---
title: Bayesian Methods for ML - 03 Latent Variable Models
date: 2019-06-03 10:03:00
categories:
  - machine learning
tags:
  - statistical machine learning
published: true
---

# Latent Variable Models

Latent variable이란 training이나 test에서 관찰 불가능한 숨겨진 변수를 의미한다.

길이나 높이같은것은 측정할 수 있지만 그렇지 못한것도 많다.

이러한 누락된 값이 존재하고, 그 불확실성을 정량화하기 위해 데이터의 probabilistic model을 필요로한다.

확률모델을 만들 때 어떤 확률변수가 다른 확률변수에 어떤 상관관계를 맺는지를 생각해야 한다.

![1](/assets/figures/ML/BM/201.JPG)

이와같이 모든 확률변수가 서로 연관이 있다고 해보자. 이와같이 선형함수를 지수로 하는 것을 정규화상수를 나누는 식으로 모델링하면 복잡도가 낮아진다. 그러나 정규화상수를 계산하는것이 쉽지 않다.

![1](/assets/figures/ML/BM/202.JPG)

우리는 '지능'과 같은 숨겨진 변수를 집어넣을 수 있다.

![1](/assets/figures/ML/BM/203.JPG)

이러한 방식으로 우리는 유연성을 줄이지 않고 모델할 수 있게 된다.

### Pros

- Simpler models(less edges)
- Fewer parameters
- Latent variables are sometimes meaningful

### Cons

- Harder to work with

---

# Probabilistic clustering

Hard clustering. 여기 속하는 거 아니면 다른 곳에 속함.

Soft clustering. 여기 몇 % 정도 속하고, 저기 몇 %정도 속하고.. 어떤 특정한 클러스터에 속하는 것이 아니라 모든 곳에 속하지만 정도의 차이가 있을 뿐.

$$p($$cluster idx \| $$x)$$

indstead of

cluster idx=$$f(x)$$

이렇게 하는 이유는?

먼저 missing data를 자연스럽게 다룰 수 있기 때문이다.

다음으로 clustering을 확률적으로 다룸으로써 hyperparemeter를 튜닝할 수 있다.

training set에서는 cluster의 수를 올릴수록 좋지만, validation set에서는 어떤 적정한 지점 이후로는 점점 log likelihood가 떨어진다.

이런건 hard clustering에서는 불가능하다.

![1](/assets/figures/ML/BM/204.JPG)

k-means의 경우 training set과 validation set 모두에서 cluster의 수가 늘어날 수록 loss도 줄어들어서 의미가 없다.

그리고 우리는 data의 generative model을 만들 수 있다.

## Summary

- Allows to tune hyper parameters
- Generative model of the data

---

# Gaussian Mixture Model

---

# Training GMM

---

# Example of GMM training
