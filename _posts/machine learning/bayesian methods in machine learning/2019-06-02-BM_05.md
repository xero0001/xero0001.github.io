---
title: Bayesian Methods for ML - 05 Applications and examples
date: 2019-06-02 10:05:00
categories:
  - machine learning
tags:
  - statistical machine learning
published: true
---

# General EM for GMM

GMM에 EM을 적용해보자.

GMM에서 파라미터 $$\theta$$에 해당하는 것은 weight인 $$\pi$$, 위치인 $$\mu$$, 공분산인 $$\sum$$이다.

![](/assets/figures/ML/BM/501.JPG)

![](/assets/figures/ML/BM/502.JPG)

![](/assets/figures/ML/BM/503.JPG)

$$\mu$$와 $$\sigma$$는 위와같이 구해진다.

여기서 $$\pi$$의 constraint를 살펴보아야한다.

합이 1이되게 하는 부분에서 Normalization용으로 object의 갯수인 N을 사용한다.

_가우시안의 weight가 q의 weight의 합에 비례하는것은 뭔가 직관적인것같다._

---

# K-means from probabilistic perspective

K-means는 하나의 포인트가 하나의 클러스터에만 속한다.

이는 Hard clustering이다.

![](/assets/figures/ML/BM/504.JPG)

![](/assets/figures/ML/BM/505.JPG)

![](/assets/figures/ML/BM/506.JPG)

hard clustering에선 q가 그냥 제일 weight 높은거 하나에 몰빵하는 식이다.

![](/assets/figures/ML/BM/507.JPG)

위는 다음 문제를 푸는것과 같다.

![](/assets/figures/ML/BM/508.JPG)
