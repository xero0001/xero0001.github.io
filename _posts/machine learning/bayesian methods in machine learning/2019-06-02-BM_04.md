---
title: Bayesian Methods for ML - 04 Expectation Maximization Algorithm
date: 2019-06-02 10:04:00
categories:
  - machine learning
tags:
  - statistical machine learning
published: true
---

# Jensen's inequality & Kullback Leibler divergence

EM 알고리즘은 거의 대부분의 latent variable model을 학습시킬 수 있다.

그 전에 몇가지 수학적 도구들을 살펴보자.

## Concave functions

이는 Jensen's inequality와 연관된다.

concave의 정의는 곡선 상의 어떤 점 두개를 직선으로 그었을 때 그 구간 사이의 곡선이 전부 직선보다 커야한다는 것.

![1](/assets/figures/ML/BM/401.JPG)

## Jensen's inequality

이는 여러개의 점으로 확장될 수 있다.

![1](/assets/figures/ML/BM/402.JPG)

Stat110 Inequality 파트 참고

## Kullback-Leibler divergence

![1](/assets/figures/ML/BM/403.JPG)

양쪽에서 두 분포는 1 차이가 나지만 variance의 차이가 있다.

그리고 파랑과 주황의 분포보다 초록과 빨강의 분포가 더 비슷해보인다.

이러한 경우 두 분포간의 차이를 나타내기 위해 사용하는 방식이 Kullback-Leibler divergence이다.

아래의 식은 $$D_{KL}(q\|p) = E[log q(x) - log p(x)]$$

와 같이 쓸 수 있다.

![1](/assets/figures/ML/BM/404.JPG)

KL divergence의 특징 세가지를 잘 기억하자.

KL divergence는 두 분포를 비교하기 위해 사용하는 것이다.

---

# Expectation-Maximization Algorithm

---

# E-step details

---

# M-step details

---

# Example: EM for discrete mixture, E-step

---

# Example: EM for discrete mixture, M-step

---

# Example: Summary of Expectation Maximization