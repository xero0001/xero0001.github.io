---
title: Logit과 Logistic, Softmax Function, Cross Entropy
date: 2019-07-09 10:06:00
categories:
  - machine learning
tags:
published: true
---

# Logit

Logit은 $$log(\frac{p(x)}{1-p(x)})$$를 의미한다.

x가 일어날 확률이 그렇지 않을 확률의 몇배나 되는가에 대한 log이다.

log를 붙이지 않은 경우 odds ratio라고 부른다.

odds ratio는 굉장히 직관적인데, odds ratio가 4이면,

다른 사건이 한번 일어난다면 그 사건은 4번 일어나는 정도로 예측하는 것이다.

이는 모두 Bernoulli 적인 얘기이다.

---

# Logistic Function

Sigmoid Function이라고도 한다.

이는 Logit의 역함수와 같다.

$$\sigma (x) = \frac{1}{1+e^{-x}}$$

## Logit of Logistic Regression

Logistic Regression에서 Logistic Function값은 그 자체로 확률과 직결된다.

Binary Classification에서 한쪽에 속할 확률을 $$\sigma (x)$$ 라고 하면, 그렇지 않을 확률은 늘 $$1 - \sigma (x)$$ 로 그 합이 1로 일정하기 때문이다.

Logistic Function을 확률로써 사용하였을 때의 또다시 그의 Logit은 다음과 같이 간단히 표현된다.

$$z=wx+b$$ 라고 할 때,

$$\frac{\sigma (z)}{1-\sigma (z)} = e^z$$

## Cross Entropy

Cross Entropy에서는 각 example 마다 $$-log(p(x))$$ 인 엔트로피를 평균낸다.

$$C(\sigma (z),y)=ylog(\sigma (z))-(1-y)log(1-\sigma (z))$$

이를 각 example마다 다 더한뒤, example 갯수만큼 나눠서 평균을 낸다.

---

# Softmax Function

Softmax Function은 Multinomial Classification으로 N개의 카테고리로 분류하기 위해 N개의 Logistic Function을 사용한다.

당연히 Logistic Function이 여러개이기 때문에 그 합이 1로 유지되지 않는데, 그렇기 때문에 Logit을 Normalize한 값을 확률로써 사용한다.

## Logit of Softmax Function

첫번째 카테고리에 대한 Logit(해당 Logistic Function이 보기에 해당 카테고리에 속한다고 믿는 확률 / 해당 카테고리에 속하지 않는다고 믿는 확률)은 다음과 같다.

$$e^{z_1}$$

## Cross Entropy

첫번째 카테고리에 대한 확률은 모든 Logistic Function의 Logit의 합을 나눠줌으로써 Normalize하여 전체 카테고리에 대해 합이 1이 되도록 하는 것으로 구한다.

$$C(z,y)=\frac{e^{z_1}}{\sum e^z}$$

이렇게 확률을 구했으면 마찬가지로 각 example에 대해 Entropy를 구하고, 이를 다 더해주고, 갯수만큼 나눠주면 되는것이다.

---

# 두 경우의 Cross Entropy는 정말 동일한가?

일단 확률 분포가 같다면, Entropy는 동일하게 적용되므로 Cross Entropy도 동일할 것이다.

물론 이를 비교하기 위해선 Binomial 버전의 Softmax를 먼저 정의해야하고, 이에 대해서 둘을 비교해보아야 할 것이다.

그리고 Binomial 버전의 Softmax와 그냥 Logistic Regression이 동일한 확률 분포를 가지기 위해서 같은 종류의 Logistic Function, 혹은 그 대칭이 되는 Logistic Function을 사용하는 것만으로도 충분한지를 살펴보아야한다.

실제로는 가장 기본적인 Sigmoid Function을 이용하여 비교하였을 때, 이 둘이 같아지기 위해선 z가 1/2로 스케일링 되어야한다.

Logistic Regression에 대해서는,

$$\sigma (x) = \frac{1}{1+e^{-x}}$$

를 사용하고,

Softmax Regression에 대해서는,

$$\sigma_1 (x) = \frac{1}{1+e^{-x}}$$

$$\sigma_2 (x) = \frac{1}{1+e^{x}}$$

를 사용해서 비교하였을 때 둘이 다르게 나오는 것이다.

이에 대한 Softmax를 계산해보면 $$\frac{1}{1+e^{2x}}$$ 와 같이 나오는데, 이것 또한 sigmoid function이므로 특정 조건을 만족하면 Logistic Regression과 완벽하게 동일한 확률분포를 얻을 수 있다.
