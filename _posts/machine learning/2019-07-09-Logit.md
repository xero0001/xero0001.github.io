---
title: Logit과 Logistic, Softmax Function, Cross Entropy
date: 2019-07-09 10:06:00
categories:
  - machine learning
tags:
published: true
---

# Logit

Logit은 $$log(\frac{p(x)}{1-p(x)})$$를 의미한다.

x가 일어날 확률이 그렇지 않을 확률의 몇배나 되는가에 대한 log이다.

---

# Logistic Function

Sigmoid Function이라고도 한다.

이는 Logit의 역함수와 같다.

$$\sigma (x) = \frac{1}{1+e^{-x}}$$

## Logit of Logistic Function

Logistic Regression에서 Logistic Function값은 그 자체로 확률과 직결된다.

Binary Classification에서 한쪽에 속할 확률을 $$\sigma (x)$$ 라고 하면, 그렇지 않을 확률은 늘 $$1 - \sigma (x)$$ 로 그 합이 1로 일정하기 때문이다.

Logistic Function을 확률로써 사용하였을 때의 또다시 그의 Logit은 다음과 같이 간단히 표현된다.

$$\frac{\sigma (x)}{1-\sigma (x)} = e^x$$

## Cross Entropy

Cross Entropy에서는 $$-log(p(x))$$ 인 엔트로피를 가중평균낸다.

$$C(\sigma (x),y)=ylog(\sigma (x))-(1-y)log(1-\sigma (x))$$
