---
title: Math&Stat for Data Science - 01 데이터 사이언스 필수 수학
date: 2019-04-27 12:10:00
categories:
- math, statistics
tags:
- [analysis, linear-algebra, optimization]
published: false
---

# 머신 러닝에 수학이 필요한 이유

"...machine learning should really be viewed as a set of techniques for leveraging data." - Aaron Hertzmann, [Intro to Bayesian learning]

머신러닝은 데이터에서 패턴을 찾아내는 테크닉들의 일부일 뿐.

이를 이해하기 위한 기본적인 수학은 필수.

함수론(해석학), 벡터론, 최적화

# 데이터 사이언스의 정의
데이터에서 (기존의 통계학으로 못 찾는 패턴을) 찾아내는 작업.

### 선형 함수 vs 비선형 함수
* 데이터를 설명하는 규칙(패턴)을 찾아내고, 이를 "학습"하는 작업.
* 작업 대상 데이터가 전체 데이터의 샘플이고, 그 샘플과 유사한 패턴이 계속해서 반복될 것이라고 가정 - 이는 통계학에서의 가정과 동일하다.
* 어떤 함수가 주어진 데이터를 더 잘 설명하는 패턴일까? - 직선이 가장 효율적이고, 데이터를 잘 설명한다고 생각한다면, 이는 통계학의 영역이다. 그러나 곡선이 더 효율적이고, 데이터를 잘 설명한다고 하면, 이는 좀 어렵다. 얼마나 복잡한 곡선이 더 잘 설명하는 것일까? 딱 맞아떨어지는 정답이 없다. 무조건 데이터에 잘 맞아떨어지면 정답이라고 한다면?
* 데이터 속에 복잡한 규칙이 숨겨져 있을 때, 최적의 (비)선형 함수를 찾아내는 작업 => 데이터 사이언스가 이러한 것을 해결함. 이러한 테크닉은 데이터 마이닝에서 공부할 수 있고, 공학에서는 이를 머신러닝이라고 부른다.
* 머신러닝은 데이터의 규칙을 찾기 위한 하나의 도구.

### 비선형 패턴
기존의 통계학이 다루던 데이터는 기본적으로 정규분포 형태의 데이터라는 가정을 깔고, fitting이 가장 잘 되는 선형함수를 찾는 작업이었다.

그러나 다양한 행동패턴을 포함하고 있는 빅데이터는 더 이상 정규분포 조건을 만족시키지 않고, 따라서 선형함수로는 최적의 설명을 할 수 없다.

머신러닝은 데이터 속에 숨겨진 규칙들을 찾아내는 모델링 작업의 일환이므로, 비선형함수 추정에서 그 효과를 크게 발휘할 수 있다.

비선형 패턴이 복잡해질 수록, 샘플 데이터가 전체를 다 설명해내지 못할 수록 머신러닝 모델링의 지적인 도전은 어려워진다.

모델 Learning에 활용하는 샘플 데이터가 전체 Population을 잘 설명해주지 못하면 모델을 계속해서 바꿔야하는 문제점도 있다.

비선형 패턴을 찾아내기 어려운 이유
* 비선형 패턴의 종류는 무한하지만, 인간이 보유하고 있는 샘플 데이터는 전체를 아우르지 못함.
* 찾아낸 패턴은 전체(X) / 샘플(O) 데이터의 한 단면에 불과.
* 시간이 지남에 따라 데이터의 패턴이 바뀌고, 모델을 새롭게 업데이트 해야하는 경우가 자주 발생.
* 선형 모델에 비해 비선형 모델은 모델 학습에 필요한 계산비용(Computational cost)가 매우 큼.

그럼 왜 찾으려고 하는가?

데이터가 계속 반복형태를 유지할 때 이 패턴을 파악하는 것은 유용하기 때문이다.

반복되는 패턴이 만약 존재하지 않는다면? 기존의 샘플 데이터는 아무 도움이 되지 않게 된다.

이것이 데이터 사이언스가 파워를 발휘할 수 있거나 그렇지 못한 영역의 단적인 예제이다.

# 함수론
## Taylor's Expansion
비선형 함수를 찾아 내면 특정 패턴을 설명할 수 있다. 완전히 단조로운 패턴을 보여주지 않더라도, 대충 비슷한 패턴만을 보여줘도 유용하게 사용할 수 있다.

그럼 비선형 모델을 어떻게 찾아낼까?

가장 간단한 표현법은 다항식이다.

무한 차수의 다항식으로 Non-linearity를 표현하는 근사방법이 Taylor 급수이다.

심지어 sin(x), cos(x), exp(x)와 같은 초월함수도 n차 다항식으로 근사할 수 있다.

이론적으로는 n이 무한대가 되면 오차 0의 완벽한 근사치 다항식을 만들어 낼 수 있다.

실제로는 n=2, 3 정도에서 타협을 하고 나머지 항목을 오차항이라고 부른다.

ML에선 근사를 하고, 차이가 나는 부분을 오차로 설정하고, 이를 최소화 하면 최대한 근사한 함수를 얻어낼 수 있다는 원리를 활용한다.

* convex function은 아래로 볼록, concave function은 위로 볼록한 함수

## Interpolation - Extrapolation이란?
ML이 통계학을 이용해 데이터를 설명하려고 하는 부분들이 다른 항목에선 어떻게 활용되었는지, 그 부분이 어떻게 ML에서 적용되는지를 살펴본다.

은행에서는 1개월 이자율, 3개월 이자율, 6개월 이자율, 12개월 이자율 이런 식으로 띄엄띄엄한 포인트의 이자율을 알려주지 1개월 25일 이자율 같은 것은 명시하지는 않는다. 이 띄엄띄엄한 포인트를 잘 메우지 않으면 대차거래(Arbitrogy)를 이용해 이득을 보는 경우가 있다. 이를 방지하기 위해 띄엄띄엄한 두 점 사이를 가장 합리적으로 연결하는 방법을 사용하는데, 이것이 Interpolation이다. n차다항식으로 이를 연결할 수 있다.

두 점 사이를 그리는 것을 Interpolation, 두 점 밖의 부분을 그리는 것을 Extrapolation이라고 한다.

우주물리학에서는 하루에 1번씩 찍은 우주 사진에 등장하는 천체의 위치를 연결하는 궤도를 찾는 작업에 활용하고, 금융에서는 1개월, 3개월, 6개월, 1년 등등의 간헐적으로 주어진 이자율 값들을 연결하는 이자율 곡선(Yield curve)를 그리는데 활용한다.

이렇게 찾은 함수로 데이터가 없는 영역의 값을 추정하는 작업을 Extrapolation이라고 한다.

이차함수를 유일하게 근사하기 위해선 원래 세개의 점이 필요하지만, 두개의 점을 잇기 위해서 이차함수를 근사하는 방법도 있다. (이 때 다양한 이차함수 중에 어떤 것을 활용하는 것일까? 다음 이차함수와의 기울기는 연결이 잘 될까?)

총 네개의 점이 있을때, 3개의 구간을 3개의 이차함수로 연결하는 방식도 있다.

## Gradient Descent
머신러닝은 데이터를 가장 잘 설명하는 패턴을 찾는 것이다.

머신러닝의 가장 기본적인 모델인 Linear Regression의 Cost Function

내가 예측하는 함수(1차 함수)와 실제 y값과의 차이가 얼마나 작게 나오게 할 수 있을지를 추정하는 것. 가장 작게 만들어주는 파라미터를 구한다.

이를 Cost Minimization이라고 한다. 통계학에서 Cost Function은 Objective Function이다.

수학공식으로 min을 찾기가 어려울 때, 모든 값을 다 찾아내기에는 너무 Computational Cost가 크므로, 효율적인 계산을 위해 여러가지 조건을 건다.

ML이 조건을 걸어 Trial-and-error를 활용하는 방식이 Gradient Descent이다.

처음엔 랜덤한 파라미터를 주고, 이것이 작아져야 하는지, 커져야하는지에 따라 최적의 path 벡터를 생성하고, 이 path를 따라 가면 cost를 감소시킬 수 있다.

# 벡터론
## 벡터 공간의 이해
모델을 복잡하게 만들기 위한 두가지 방법.
* 다항식에서: Non-linearity 추가. \\(x^1, x^2, x^3,...\\)
* 다변수식: linear이지만, \\(x_1, x_2, x_3,...\\) 등 변수를 많이 만든다.
다른 방향처럼 보이지만 결과적으로 유사하다.

Homogeneity임을 가정함.
$$Y=X\theta+\epsilon$$
$$E[\epsilon]=0$$
$$var[\epsilon]=V_\epsilon=\sigma^2I_n$$

## 회귀분석
## Projection Matrix
## Eigen Decomposition
## PCA

# Optimization
## Kernel
## SVM
## Optimization basic
## Optimization advanced