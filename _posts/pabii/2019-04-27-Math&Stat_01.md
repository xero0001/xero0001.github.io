---
title: Math&Stat for Data Science - 01 Mathematics for Data Science
date: 2019-04-27 12:10:00
categories:
- math, statistics
tags:
- [analysis, linear-algebra, optimization]
published: false
---

# 머신 러닝에 수학이 필요한 이유

"...machine learning should really be viewed as a set of techniques for leveraging data." - Aaron Hertzmann, [Intro to Bayesian learning]

머신러닝은 데이터에서 패턴을 찾아내는 테크닉들의 일부일 뿐.

이를 이해하기 위한 기본적인 수학은 필수.

함수론(해석학), 벡터론, 최적화

# 데이터 사이언스의 정의
데이터에서 (기존의 통계학으로 못 찾는 패턴을) 찾아내는 작업.

### 선형 함수 vs 비선형 함수
* 데이터를 설명하는 규칙(패턴)을 찾아내고, 이를 "학습"하는 작업.
* 작업 대상 데이터가 전체 데이터의 샘플이고, 그 샘플과 유사한 패턴이 계속해서 반복될 것이라고 가정 - 이는 통계학에서의 가정과 동일하다.
* 어떤 함수가 주어진 데이터를 더 잘 설명하는 패턴일까? - 직선이 가장 효율적이고, 데이터를 잘 설명한다고 생각한다면, 이는 통계학의 영역이다. 그러나 곡선이 더 효율적이고, 데이터를 잘 설명한다고 하면, 이는 좀 어렵다. 얼마나 복잡한 곡선이 더 잘 설명하는 것일까? 딱 맞아떨어지는 정답이 없다. 무조건 데이터에 잘 맞아떨어지면 정답이라고 한다면?
* 데이터 속에 복잡한 규칙이 숨겨져 있을 때, 최적의 (비)선형 함수를 찾아내는 작업 => 데이터 사이언스가 이러한 것을 해결함. 이러한 테크닉은 데이터 마이닝에서 공부할 수 있고, 공학에서는 이를 머신러닝이라고 부른다.
* 머신러닝은 데이터의 규칙을 찾기 위한 하나의 도구.

### 비선형 패턴
기존의 통계학이 다루던 데이터는 기본적으로 정규분포 형태의 데이터라는 가정을 깔고, fitting이 가장 잘 되는 선형함수를 찾는 작업이었다.

그러나 다양한 행동패턴을 포함하고 있는 빅데이터는 더 이상 정규분포 조건을 만족시키지 않고, 따라서 선형함수로는 최적의 설명을 할 수 없다.

머신러닝은 데이터 속에 숨겨진 규칙들을 찾아내는 모델링 작업의 일환이므로, 비선형함수 추정에서 그 효과를 크게 발휘할 수 있다.

비선형 패턴이 복잡해질 수록, 샘플 데이터가 전체를 다 설명해내지 못할 수록 머신러닝 모델링의 지적인 도전은 어려워진다.

모델 Learning에 활용하는 샘플 데이터가 전체 Population을 잘 설명해주지 못하면 모델을 계속해서 바꿔야하는 문제점도 있다.

비선형 패턴을 찾아내기 어려운 이유
* 비선형 패턴의 종류는 무한하지만, 인간이 보유하고 있는 샘플 데이터는 전체를 아우르지 못함.
* 찾아낸 패턴은 전체(X) / 샘플(O) 데이터의 한 단면에 불과.
* 시간이 지남에 따라 데이터의 패턴이 바뀌고, 모델을 새롭게 업데이트 해야하는 경우가 자주 발생.
* 선형 모델에 비해 비선형 모델은 모델 학습에 필요한 계산비용(Computational cost)가 매우 큼.

그럼 왜 찾으려고 하는가?

데이터가 계속 반복형태를 유지할 때 이 패턴을 파악하는 것은 유용하기 때문이다.

반복되는 패턴이 만약 존재하지 않는다면? 기존의 샘플 데이터는 아무 도움이 되지 않게 된다.

이것이 데이터 사이언스가 파워를 발휘할 수 있거나 그렇지 못한 영역의 단적인 예제이다.

# 함수론
## Taylor's Expansion
비선형 함수를 찾아 내면 특정 패턴을 설명할 수 있다. 완전히 단조로운 패턴을 보여주지 않더라도, 대충 비슷한 패턴만을 보여줘도 유용하게 사용할 수 있다.

그럼 비선형 모델을 어떻게 찾아낼까?

가장 간단한 표현법은 다항식이다.

무한 차수의 다항식으로 Non-linearity를 표현하는 근사방법이 Taylor 급수이다.

심지어 sin(x), cos(x), exp(x)와 같은 초월함수도 n차 다항식으로 근사할 수 있다.

이론적으로는 n이 무한대가 되면 오차 0의 완벽한 근사치 다항식을 만들어 낼 수 있다.

실제로는 n=2, 3 정도에서 타협을 하고 나머지 항목을 오차항이라고 부른다.

ML에선 근사를 하고, 차이가 나는 부분을 오차로 설정하고, 이를 최소화 하면 최대한 근사한 함수를 얻어낼 수 있다는 원리를 활용한다.

* convex function은 아래로 볼록, concave function은 위로 볼록한 함수

## Interpolation - Extrapolation이란?
ML이 통계학을 이용해 데이터를 설명하려고 하는 부분들이 다른 항목에선 어떻게 활용되었는지, 그 부분이 어떻게 ML에서 적용되는지를 살펴본다.

은행에서는 1개월 이자율, 3개월 이자율, 6개월 이자율, 12개월 이자율 이런 식으로 띄엄띄엄한 포인트의 이자율을 알려주지 1개월 25일 이자율 같은 것은 명시하지는 않는다. 이 띄엄띄엄한 포인트를 잘 메우지 않으면 대차거래(Arbitrogy)를 이용해 이득을 보는 경우가 있다. 이를 방지하기 위해 띄엄띄엄한 두 점 사이를 가장 합리적으로 연결하는 방법을 사용하는데, 이것이 Interpolation이다. n차다항식으로 이를 연결할 수 있다.

두 점 사이를 그리는 것을 Interpolation, 두 점 밖의 부분을 그리는 것을 Extrapolation이라고 한다.

우주물리학에서는 하루에 1번씩 찍은 우주 사진에 등장하는 천체의 위치를 연결하는 궤도를 찾는 작업에 활용하고, 금융에서는 1개월, 3개월, 6개월, 1년 등등의 간헐적으로 주어진 이자율 값들을 연결하는 이자율 곡선(Yield curve)를 그리는데 활용한다.

이렇게 찾은 함수로 데이터가 없는 영역의 값을 추정하는 작업을 Extrapolation이라고 한다.

이차함수를 유일하게 근사하기 위해선 원래 세개의 점이 필요하지만, 두개의 점을 잇기 위해서 이차함수를 근사하는 방법도 있다. (이 때 다양한 이차함수 중에 어떤 것을 활용하는 것일까? 다음 이차함수와의 기울기는 연결이 잘 될까?)

총 네개의 점이 있을때, 3개의 구간을 3개의 이차함수로 연결하는 방식도 있다.

## Gradient Descent
머신러닝은 데이터를 가장 잘 설명하는 패턴을 찾는 것이다.

머신러닝의 가장 기본적인 모델인 Linear Regression의 Cost Function

내가 예측하는 함수(1차 함수)와 실제 y값과의 차이가 얼마나 작게 나오게 할 수 있을지를 추정하는 것. 가장 작게 만들어주는 파라미터를 구한다.

이를 Cost Minimization이라고 한다. 통계학에서 Cost Function은 Objective Function이다.

수학공식으로 min을 찾기가 어려울 때, 모든 값을 다 찾아내기에는 너무 Computational Cost가 크므로, 효율적인 계산을 위해 여러가지 조건을 건다.

ML이 조건을 걸어 Trial-and-error를 활용하는 방식이 Gradient Descent이다.

처음엔 랜덤한 파라미터를 주고, 이것이 작아져야 하는지, 커져야하는지에 따라 최적의 path 벡터를 생성하고, 이 path를 따라 가면 cost를 감소시킬 수 있다.

# 벡터론
## 벡터 공간의 이해
벡터 공간을 통한 회귀분석의 이해

모델을 복잡하게 만들기 위한 두가지 방법.
* 다항식에서: Non-linearity 추가. \\(x^1, x^2, x^3,...\\)
* 다변수식: linear이지만, \\(x_1, x_2, x_3,...\\) 등 변수를 많이 만든다.
다른 방향처럼 보이지만 결과적으로 유사하다.

Homogeneity임을 가정함.
$$Y=X\theta+\varepsilon$$
$$E[\varepsilon]=0$$
$$var[\varepsilon]=V_\varepsilon=\sigma^2I_n$$
ML에서는 epsilon에 크게 집중하지 않는다.
그러나 이 가정이 깨지게 되면(Heterogenity라면), 이 식이 적용되지 못한다.
그러면 이러한 방식의 학습은 Performance가 낮을 수 밖에 없다.

회귀 분석식을 벡터공간으로 표현하면  
$$y=\theta_0+\theta_1x_1+\theta2x_2+\varepsilon$$

epsilon은 아래와 같이 표현할 수 있다.  
$$\varepsilon~(0, \sigma^2I)$$  
$$\varepsilon=\alpha x_1+\beta x_2+\kappa$$  
$$\kappa~(0,\sigma_{k}^{2})$$

이 때 epsilon의 alpha와 beta가 0이 아니라면 theta값을 잘못 설정한 것이다. x1과 x2로 설명가능한 부분이 숨어있다는 뜻이기 때문이다.  
그러므로 alpha와 beta가 0이 되도록 하여야 한다.  
그러기 위해선 \\(x_1\perp\varepsilon, x_2\perp\varepsilon\\)이 성립하여야 한다.

그림에서 \\(\hat{\beta}\\) 은 theta hat으로 생각.

$$y=\hat{y}+e$$

## 회귀분석
$$h_\theta(x)+epsilon=y=\theta_0+\theta_1X+\varepsilon$$  
$$min_{\theta_0,\theta_1}(\theta_0+\theta_1X-y)^2$$  
$$Y=X\theta+\varepsilon$$  
$$[n \times 1] [n \times (p+1)] [(p+1) \times 1] [n \times 1]$$  

양변에 X' 곱하면,  
$$X'Y=X'X\theta+X'\varepsilon$$

양변에 (X'X)^-1 곱하면  
$$(X'X)^{-1}X'Y=(X'X)^{-1}X'X\theta+(X'X)^{-1}X'\varepsilon$$

전개하면, (우변의 2번째 항은 X와 epsilon이 직교하거나, 최소한 correlation이 없다면, n이 무한에 다가가면(대수의 법칙) 우변의 2번째 항도 0으로 수렴함 - 이를 가정하면)  
$$(X'X)^{-1}X'Y=I\theta+(X'X)^{-1}X'\varepsilon$$

직교하게되거나 correlation이 없어 0에 수렴하게 되면 theta에 대해 정리된다. 그리고 다음이 y를 가장 잘 설명하는 theta값이다.  
$$\hat{\theta}=(X'X)^{-1}X'y$$

이렇게 epsilon의 평균이 0이고 분산이 시그마제곱이라고 가정하는 것을 OLS(Ordinary Least Square)라고 하고, 이런 가정이 무너진 경우를 GLS(General Least Square)라고 한다.

ML에선 GLS는 알필요없다. ML에서 Normal Equation이라고 부르는 이유는 데이터의 분포가 Normal distribution일 때 선형 최적화 계수가 비선형 모델들보다 우월하기 때문.

그런데 Normal equation을 이용하기 위해선 역행렬이 존재하여야 하고, 즉 full rank여야 한다는 뜻이다. row dependency가 없어야 함. linear combination으로 다른 row를 만들 수 없어야함.

## Projection Matrix
Ax=b 시스템을 풀 때, 데이터의 축(X1, X2)으로부터 직각인 축(W1, W2)을 만들어낼 수 있다면 계산이 정말 편하지 않을까?  
Independence한(Orthogonal) 축으로 만들 수 있다면.  
Matrix A가 diagonal한 Matrix로 사용될 수 있다.

## Eigen Decomposition
Matrix가 Symmetric하고, Full Rank에 Rank숫자만큼의 distinct eigenvector가 있으면 Eigenvalue에 대한 Eigenvector들이 orthonormal한 basis를 갖는다.  
https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal

## PCA
Covariance Matrix의 Eigenvector들은 전부 직교함. Symmetric하기 때문.  
PCA는 상관관계가 높은 벡터만을 찾아내 사용하고 낮은 벡터들은 버리는 방식으로 차원을 줄인다.  
이를 이용해 적은 feature로도 높은 해석력을 낼 수 있다.  

# Optimization
## Kernel
Kernel이란 특정 데이터 셋, 혹은 어떤 함수를 다른 함수 형태로 바꿔주는 함수.  
함수 A를 함수 B의 형태로 바꿔주는 함수.

간단 예시

우리반 시험 평균 점수가 30점 밖에 나오지 않아, 학생 전원에게 +30점을 주는 경우 or 점수를 2배 해 주는 경우

Cos(x) 함수를 pi/2 만큼 우측으로 평행이동시켜 Sin(x) 함수로 만드는 경우

g(f(x))와 같은 합성함수에서 g가 하는 역할이 kernel

Activation Function도 Kernel이다.  

ReLU는 틀린 부분은 버리고 맞는 부분은 계속 쓰는 것

## SVM
Margin-Maximizer의 개념에서 제일 좋은 Hyperplane 선택


Decision Boundary & Separating Hyperplane

데이터를 2개 이상의 그룹으로 나누고 싶을 때, Decision Boundary를 이용해 데이터의 그룹을 구분하는 작업을 Classification이라고 한다.

그 중 일반적으로 가장 많이 쓰이는 계산법이 Logistic regression (약칭 Logit)과 Support Vector Machine (SVM) 이다.

Logit: 모든 데이터와 Decision Boundary의 (최단)거리를 구한 후, 거리의 제곱을 최소화해주는 Decision Boundary 선택
SVM: 데이터 그룹 (그룹1, 그룹2,…)과 Decision Boundary의 거리를 구해서, 거리의 합이 최대가 되도록 선택 (Maring Maximizer)
SVM은 두 개 그룹으로 공간을 구분하는 작업이기 때문에 SVM의 Decision Boundary를 Separating Hyperplane 이라고 부름
선형대수학 교과서의 Separating Hyperplane Theorem 섹션에서 N차원의 공간은 반드시 N-1차원의 Separating Hyperplane을 가진 다는 증명을 확인해볼것 - Separating Hyperplane Theorem(컨벡스 최적화에서도 나옴)
둘의 가장 큰 차이점은 데이터와 Decision Boundary간 거리를 낱개의 데이터에서 구하느냐, 0과 1의 데이터 그룹으로 구하느냐
 

Overfitting Problem

데이터 포인트 각각을 이용해서 Decision Boundary를 찾을 때는 데이터가 1/n의 가중치를 갖게 되지만, 그룹으로 묶은 다음 Decision Boundary를 찾을 때는 가장 가까운 점에게 모든 가중치가 몰린다. 강의 노트의 예시에서 노란선이 Maximum Margin Classifier (SVM의 또 다른 이름) 이겠지만, Outlier의 위치에 따라 검은색 선이 선택될 수도 있다. 그 경우 Training data에서만 잘 맞고, Test data에서는 엉망이 되는 Overfitting 문제가 발생할 가능성이 높아진다.

 

Kernel & Non-linearity

SVM은 항상 Separating Hyperplace을 이용해 데이터 그룹을 구분하기 때문에 직선, 평면 같은 공간만 쓸 수 있어서 Logit의 Non-linearity를 구현하지 못한다. 때문에 Kernel을 이용해 공간을 뒤틀어 버린다음, 새로운 공간에서 직선, 평면 같은 Hyperplane을 찾고, 공간 왜곡을 원상 복구 시키면 Hyperplane이 Non-linearity를 갖도록 할 수 있다.

 

(Un)Constrained Optimization

데이터 그룹을 구분하는 Hyperplane이 두 그룹간의 거리를 최대화해도록 만들어야하므로, wX + b 라는 Hyperplane이 두 영역과 닿는 거리를 최대화하는 조건식을 만족하는 (w, b) 순서쌍을 찾는 계산이 바로 SVM 계산이 된다.

후술하는 Lagrangian 기법, Kuhn-Tucker 기법을 이용해 Unconstrained 예제와 Constrained 예제를 풀어볼 수 있으나, 프로그램으로 구현하는데 제약사항이 많아서, 수학적으로 동치인 다른 계산을 활용하는 방법 (Duality)으로 실제 SVM 계산을 하게 된다.

* Duality

원과 직선의 최단거리를 원과 직선간의 접점이다. 이 때 원의 위치를 움직이거나, 직선의 위치를 움직이거나 상관없이 최단거리 값은 동일하다는 개념이라고 접근하면 된다. SVM에서는 Constrained 조건식을 목적함수로 놓고, 원래 목적함수를 Unconstrained 조건식으로 바꾸는 방법으로 활용한다. SVM 적용에 대한 자세한 내용은 데이터 사이언스 기본 강좌의 SVM 편에서 자세하게 다루게 된다.

## Optimization basic
헤시안 행렬

f(x)=*
f_x f_xx

f(x,y)=*

f_xx f_xy
f_xy f_yy

의 determinant

f_xx
f_xxf_yy - (f_xy)^2

첫번째 양수, 두번째 양수면 positive definite
equality가 들어가면 positive semi definite

첫번째 음수, 두번째 양수면 negative definite
equality가 들어가면 negative semi definite

negative definite이면 최대값과 최솟값을 둘다 가진다. ex) 3차함수- 변곡점 양옆으로 최댓값 최솟값

wx+b=0 같은 조건식이 추가되면? - 람다

라그랑지안 L = f(x,y) + lambda(g(x,y)-b)

이 경우 헤시안은
f_xx f_xy f_xlambda
f_yx f_yy f_ylambda
f_lx f_ly f_ll

determinant 구하면

세개 다 양수면 pd
음수, 양수, 음수면 nd

## Optimization advanced
g(x,m)=x+m
b = 4

x+m-4 <0 이어야 하기 때문에,
~~-lambda(X+M-4)
로 이 항을 양수로 만들어줌  
라그랑지안 할땐 양수로 해주어야한다  

원의 접선같은경우
람다 >0
x+m=4

두 직선의 교점
람다 = 0
x+m<4

equality는 lagrangian -> 이것도 복잡

inequality는 KT-method
이는 SVM을 풀때 사용

등호인 조건식이 있을 때 (Unconstrained optimization case)

Lagrangian method를 이용해 조건식을 만족시키는 목적함수의 최대/최소값을 구하는 작업
Hessian matrix를 이용해 최대/최소값이 실제로 존재하는지 확인하는 작업을 거친다
 

부등호인 조건식이 있을 때 (Constrained optimization case)

Kuhn-Tucker method를 이용해 조건식의 부등호를 만족시키는 목적함수의 최대/최소값을 구하는 작업
경우의 수를 따져야하기 때문에 일반적인 Computer program으로 계산을 처리하기 복잡해짐
SVM은 위의 문제를 회피하기 위해 Duality를 이용해 조건식과 목적함수를 뒤집어서 계산
(식 변화와 관련된 자세한 내용은 데이터 사이언스 기본 강좌에서 확인)