---
title: Linear Algebra - UNIT III - 08 Singular Value Decomposition
date: 2019-05-06 15:30:00
categories:
  - mathematics
tags:
  - linear algebra
---

Gilbert Strang - 짧은버전  
[Learn Differential Equations: SVD](https://www.youtube.com/watch?v=mBcLRGuAFUk)

Gilbert Strang - 풀버전  
[Linear Algebra: Lec. 29](https://www.youtube.com/watch?v=Nx0lRBaXoz4&t=700s)

[SVD summary pdf](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf)

블로그  
[다크프로그래머](https://darkpgmr.tistory.com/106)

---

# Singular Value Decomposition

강의에서 다루는 마지막이자 최고의 factorization이다!

## What is Singular Value Decomposition?

$$A= U \Sigma V^T$$

우변은 각각 Orthogonal, Diagonal, Orthogonal Matrix로 구성된다.

Orthogonal과 Diagonal 둘다 정말 좋은 특성을 지니고 있다.

Orthogonal은 역행렬이 간단하게 정리된다.  
Diagonal은 제곱계산에 대해 간단하다.

**A는 어떤 행렬이어도 상관없다. 어떠한 A라도 SVD를 가지고 있다.**

새로운 점은 우리가 서로 다른 두개의 orthogonal matrices가 필요하다는 것이다.

이 분해는 이 코스의 모든 것을 함축한다.

### Symmetric Positive Definitive

먼저 불러올 것은 PD이다.

$$A=Q \Lambda Q^T$$

$$A=S \Lambda S^{-1}$$

Symmetric한 행렬은 Eigenvector가 Orthogonal하다. 위에서 일반적인 행렬인 S가 Q가 된 것을 볼 수 있다.

그리고 PD에서 일반적인 $$\Lambda$$가 Positive $$\Lambda$$가 된 것이다.

$$A=Q \Lambda Q^T$$

이것은 행렬이 Symmetric Positive Definite일 경우 Singular Value Decomposition을 한 것과 같다.

Symmetric Positive Definite에서는 U나 V가 필요가 없다. 그냥 하나의 Orthogonal matrix면 충분하다.

그러나 다음에는 해당하지 않는다.

$$A=S \Lambda S^{-1}$$

일반적으로 S 행렬은 orthogonal하지 않다. 그래서 이는 신경쓰지 않겠다.

Orthogonal x Diagonal x Orthogonal

인 경우를 생각하는것이 좋다.

이게 어떤 의미를 가지는지, 어디서 온것인지 알아보자.

## How it works

![SVD](http://alexeygrigorev.com/projects/imsem-ws14-lina/img-svg/diagram3-svd.svg)

우린 행렬 A를 row space의 벡터 $$v_1$$을 column space의 벡터

$$u_1 = A v_1$$

으로 보내는 것으로 생각할 수 있다.

SVD는 row space에 대한 orthogonal basis를 column space에 대한 orthogonal basis로 변환하는 것에서 출발한다.

$$Av_i = \sigma_i u_i$$

Row space에 대한 orthogonal basis를 찾는 것은 어렵지 않은 일이다 - Gram Schmidt process를 이용한다면.  
그러나 그 orthogonal basis를 또 다른 공간의 orthogonal basis를 보낸다고 그것이 orthogonal이라는 보장이 없다.  
그렇기 때문에 특별한 setup이 필요하다.

$$A,A^T$$의 Nullspace상에 있는 벡터들에 대해 걱정할지도 모르지만 상관없다.  
$$\Sigma$$의 대각성분에 있는 0으로 나타날 것이기 때문이다.

## Matrix language

Orthogonal할 뿐만 아니라 orthonormal하게 해보자.

$$
A
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
=
\begin{bmatrix}
\sigma_1 u_1 & \sigma_2u_2 & \cdots & \sigma_ru_r
\end{bmatrix}
\\\\
\begin{bmatrix}
u_1 & u_2 & \cdots & u_r
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & & & \\
 & \sigma_2 & & \\
 &  & \ddots & \\
 &  & & \sigma_r \\
\end{bmatrix}
$$

문제의 핵심은 행렬 A에 대해 column space상의 orthonormal basis $$u_1,u_2,...u_r$$로 변환 되는 row space상의 orthonormal basis $$v_1,v_2,...v_r$$를 찾는 것이다. Nullspace를 더하면 이 방정식은 $$AV=U \Sigma$$가 된다.
