---
title: Linear Algebra - UNIT III - 08 Singular Value Decomposition
date: 2019-05-06 15:30:00
categories:
  - mathematics
tags:
  - linear algebra
---

Gilbert Strang - 짧은버전  
[Learn Differential Equations: SVD](https://www.youtube.com/watch?v=mBcLRGuAFUk)

Gilbert Strang - 풀버전  
[Linear Algebra: Lec. 29](https://www.youtube.com/watch?v=Nx0lRBaXoz4&t=700s)

[SVD summary pdf](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf)

블로그  
[다크프로그래머](https://darkpgmr.tistory.com/106)

---

# Singular Value Decomposition

강의에서 다루는 마지막이자 최고의 factorization이다!

## What is Singular Value Decomposition?

$$A= U \Sigma V^T$$

우변은 각각 Orthogonal, Diagonal, Orthogonal Matrix로 구성된다.

Orthogonal과 Diagonal 둘다 정말 좋은 특성을 지니고 있다.

Orthogonal은 역행렬이 간단하게 정리된다.  
Diagonal은 제곱계산에 대해 간단하다.
(+Symmetric한 행렬은 eigenvector가 orthogonal하다.)

**A는 어떤 행렬이어도 상관없다. 어떠한 A라도 SVD를 가지고 있다.**

새로운 점은 우리가 서로 다른 두개의 orthogonal matrices가 필요하다는 것이다.

이 분해는 이 코스의 모든 것을 함축한다.

### Symmetric Positive Definitive

먼저 불러올 것은 PD이다.

$$A=Q \Lambda Q^T$$

$$A=S \Lambda S^{-1}$$

Symmetric한 행렬은 Eigenvector가 Orthogonal하다. 위에서 일반적인 행렬인 S가 Q가 된 것을 볼 수 있다.

그리고 PD에서 일반적인 $$\Lambda$$가 Positive $$\Lambda$$가 된 것이다.

$$A=Q \Lambda Q^T$$

행렬이 Symmetric Positive Definite일 경우 위 식은 Singular Value Decomposition을 한 것과 같다.  
Symmetric Positive Definite에서는 U나 V가 필요가 없다. 그냥 하나의 Orthogonal matrix면 충분하다.

그러나 다음에는 해당하지 않는다.

$$A=S \Lambda S^{-1}$$

일반적으로 S 행렬은 orthogonal하지 않다. 그래서 이는 신경쓰지 않겠다.

Orthogonal x Diagonal x Orthogonal

인 경우를 생각하는것이 편하다.

이게 어떤 의미를 가지는지, 어디서 온것인지 알아보자.

## How it works

![SVD](http://alexeygrigorev.com/projects/imsem-ws14-lina/img-svg/diagram3-svd.svg)

우린 행렬 A를 row space $$R^n$$의 벡터 $$v_1$$을 column space $$R^m$$의 벡터

$$u_1 = A v_1$$

으로 보내는 것으로 생각할 수 있다.

SVD는 row space에 대한 orthogonal basis를 column space에 대한 orthogonal basis로 변환하는 것에서 출발한다.  
$$u$$를 단위벡터로 만들어 orthonormal하게 표현하기 위해 다음처럼 $$\sigma$$를 추가하여 스케일링 해준다.

$$Av_i = \sigma_i u_i$$

Row space에 대한 orthonormal basis를 찾는 것은 어렵지 않은 일이다 - Gram Schmidt process를 이용한다면.  
그러나 그 orthogonal basis를 또 다른 공간의 orthogonal basis를 보낸다고 그것이 orthogonal이라는 보장이 없다.  
그렇기 때문에 특별한 setup이 필요하다.

$$A,A^T$$의 Nullspace상에 있는 벡터들에 대해 걱정할지도 모르지만 상관없다.  
$$\Sigma$$의 대각성분 $$\sigma_{r+1},\sigma_{r+2},...\sigma_{n}$$는 0으로 나타날 것이기 때문이다.  
그러므로 표시하지 않아도 된다.

## Matrix language

이를 행렬식으로 표현해보자.

앞으로 $$\Lambda$$대신에 $$\Sigma$$라고 부를것이다.

$$
A
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
=
\begin{bmatrix}
\sigma_1 u_1 & \sigma_2u_2 & \cdots & \sigma_ru_r
\end{bmatrix}
\\\\
=
\begin{bmatrix}
u_1 & u_2 & \cdots & u_r
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & & & \\
 & \sigma_2 & & \\
 &  & \ddots & \\
 &  & & \sigma_r \\
\end{bmatrix}
$$

$$\sigma$$는 단위벡터 $$u$$를 스케일링하기 위한 것이다.

문제의 핵심은 행렬 A에 대해 column space상의 orthonormal basis $$u_1,u_2,...u_r$$로 변환 되는 row space상의 orthonormal basis $$v_1,v_2,...v_r$$를 찾는 것이다.

Null space까지 포함해서 이를 다시 정리하면

$$AV=U \Sigma$$

가 된다.

Orthonormal basis $$V$$ in row space, orthonormal basis $$U$$ in column space. 이 둘을 이용하여 행렬을 대각화하였다.  
행렬 A가 대각화된 행렬로 표현된 것이다.

일반적으로는 U와 V는 서로 다른 행렬이다.  
그러나 Positive definite의 경우는

$$AQ=Q \Sigma$$

인 경우다.  
V와 U가 똑같이 Q인 것이다.  
즉, 같은 basis를 row 와 column space에 사용할 수 있다.

## Calculation

$$
A
=
\begin{bmatrix}
4 & 4 \\
-3 & 3
\end{bmatrix}
$$

이는 invertible하고 rank 2를 가진다.  
그러나 symmetric하지 않아서 eigenvector가 orthogonal하지 않아 그대로 이용할 수 없다.

row space $$R^2$$에서 $$v_1,v_2$$를 찾고, column space $$R^2$$에서 $$u_1,u_2$$를 찾아보자.  
그리고 $$\sigma_1 \gt 0,\sigma_2 \gt 0$$를 찾아 $$v_i,u_i$$을 orthonormal하게 만들자.

다음에 대한 Orthonormal 행렬 V,U, Diagonal 행렬 $$\Sigma$$를 찾아보자.

$$AV=U \Sigma$$

$$V$$가 orthogonal하므로 $$V^{-1}=V^T$$ 로 양변에 곱한다.

$$A=U \Sigma V^T$$

$$U,V$$와 $$\Sigma$$ 를 동시에 풀기보단, 양변에 $$A^T=V \Sigma^T U^T$$를 곱하여 U를 없애자:

$$
A^TA = V \Sigma U^{-1} U \Sigma V^T \\\\
=V \Sigma^2 V^T \\\\
= V
\begin{bmatrix}
\sigma_1^2 & & & \\
 & \sigma_2^2 & & \\
& & \ddots & \\
 & & & \sigma_n^2 \\
\end{bmatrix}
V^T
$$

$$A^TA$$는 Symmetric하며, positive definite 혹은 semidefinite이다.

이것은 $$Q \Lambda Q^T$$형태라는 것을 알 수 있다.  
그러므로 행렬 $$A^TA$$를 대각화함으로써 V를 찾을 수 있다.  
$$V$$의 column들은 $$A^TA$$의 eigenvector들이고, positive definite이기 때문에 $$\sigma_i^2$$는 양수인 eigenvalue들이다.($$\sigma_i$$는 양수인 $$\lambda_i$$의 제곱근을 선택하면 된다.)

$$U$$를 얻기 위해서 우리는 $$AA^T$$에 대해 같은 것을 하면된다.

## SVD example

$$
A=
\begin{bmatrix}
4 & 4 \\
-3 & 3
\end{bmatrix}
$$

다시 돌아와서 계산해보자.

$$
A^TA =
\begin{bmatrix}
25 & 7 \\
7 & 25
\end{bmatrix}
$$

Eigenvector는 $$v_i$$가 될것이고, eigenvalue는 $$\sigma_i$$가 될것이다.

$$
v_1=
\begin{bmatrix}
1/\sqrt{2}\\
1/\sqrt{2}
\end{bmatrix}
, v_2=
\begin{bmatrix}
1/\sqrt{2}\\
-1/\sqrt{2}
\end{bmatrix}
$$

$$\sigma_1^2=32,\sigma_2^2=18$$

$$A=U \Sigma V^T$$

$$
\begin{bmatrix}
4 & 4\\
-3 & 3
\end{bmatrix}
=
\begin{bmatrix}
 \\

\end{bmatrix}
\begin{bmatrix}
4 \sqrt{2} & 0\\
0 & 3 \sqrt{2}
\end{bmatrix}
\begin{bmatrix}
1/\sqrt{2} & 1/\sqrt{2}\\
1/\sqrt{2} & -1/\sqrt{2}
\end{bmatrix}
$$

이를 통해 U를 풀 수 있으나, 연습을 위해 $$AA^T$$로도 해보면 좋다.

풀이중 $$A^TA$$의 eigenvalue와 $$AA^T$$의 eigenvalue가 같다는 언급.

$$
U=
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
}
$$

## Example with a nullspace

$$
A=
\begin{bmatrix}
4 & 3 \\
8 & 6
\end{bmatrix}
$$
