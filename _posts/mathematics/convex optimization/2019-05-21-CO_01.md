---
title: Convex Optimization - 01 Optimization in Machine Learning and Statistics
date: 2019-05-21 10:01:00
categories:
  - mathematics
tags:
  - linear algebra
---

[2016 Lec Videos](https://www.youtube.com/watch?v=XFKBNJ14UmY&feature=youtu.be)

[2018 Lec Videos](https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1bd9caa1-82ab-49b1-ab5d-a949011e4fdc)

[2016 page](http://www.stat.cmu.edu/~ryantibs/convexopt-F16/)

[2018 page](http://www.stat.cmu.edu/~ryantibs/convexopt/)

[2016 Slides](http://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/intro.pdf)

[2016 Notes](http://www.stat.cmu.edu/~ryantibs/convexopt-F16/notes/intro-notes.pdf)

---

# Optimization problems

"Translate **Conceptual idea** into $$P:min_{x \in D} f(x)$$, which is Optimization problem"

Convex optimization에서는 P를 푸는 방법에 대해 배운다.

## Examples of opt problems

- Network flow
- Regressions: least squares - $$min_{\Beta} (y_i-x_i^T \Beta)^2$$
- Regularized regression: lasso - $$min_{\Beta} (y_i-x_i^T \Beta)^2$$ s.t. $$\sum |\Beta_j| \le t$$
- Max grade s.t. $$effot \lt t$$
- Max likelihood
- SVMs
- Classification: logistic regression

...

## Examples of contrary

- [hypothesis testing / p-values](https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/more-significance-testing-videos/v/hypothesis-testing-and-p-values)
- boosting
- random forests
- cross-validation/bootstrap

## Why?

이미 다른사람들이 $$P: min_{x \in D} f(x)$$를 푸는 법을 다 알아 냈을텐데, 왜 힘들이는가.

두가지만 말해보자면,

1. 서로 다른 알고리즘은 서로다른 문제 P에 대해서 성능이 다를 수 있다.(언제나 최고인 알고리즘은 없다)
2. P를 공부하는 것은 문제에 대한 통계적 과정을 이해하는데 깊은 이해를 준다.

최적화는 현재도 계속해서 발전중인 분야다. 빠르게 변화할 수 있다. 하지만 통계학과 ML에 대해 발전할 부분이 많이 남아있다.

---

# Example: algorithms for 2d fused lasso

2d fused lasso or 2d total variation denoising:

$$\overbrace{min}_{\theta} \frac{1}{2} \sum_{i=1}^{n} (y_i - \theta_i)^2 + \lambda \sum_{(i,j) \in E} |\theta_i - \theta_j|$$

![1-1](/assets/figures/CO/1-1.PNG)

_강의노트 참고_

y는 [3,7] 값을 가지는 우리가 실제로 보는 Data 이미지를 벡터로 unravel한 것이다.

실제 이미지는 구간별 상수(piecewise constant)를 가진다.  
이미지의 y가 자주 바뀌지 않는다.

우리가 보는 데이터는 노이즈가 굉장히 심하다. 더이상 piecewise const하지않다.

우린 데이터 엔트리 $$y_i$$에 대해 $$\theta$$ 하나만을 가지고 있다. 이 역시 이미지를 벡터로 unravel한것이다.

첫번째 항은 least square loss이고, theta가 원래 데이터와 너무 달라지지 않도록 하는 것이다.

두번째 항은 페널티인데, $$\lambda$$는 하이퍼파라미터인데 25정도로 크게 설정했다고 한다. 그리고 $$i,j$$는 모든 인접한 theta이다. E는 edge의 집합이다. 인접한 데이터가 서로 다르면 페널티도 커진다. $$\lambda$$를 크게 설정하면 i,j가 많은 곳에서 완전히 똑같아진다. 결국 인접 픽셀을 똑같은 색으로 만드는 역할을 한다. 람다가 커질수록 이미지가 piecewise constant해진다. 무한하게 크다면 이미지 전체가 Y의 평균색으로 변할것이다.

...

서로 다른 알고리즘들을 통해서 푼결과들에 대한 얘기.

## 의미

여기서 alternating direction method of multiplier(ADMM)이 다른 메소드들을 발라버렸다.

그러면 ADMM이 최고의 알고리즘인가?

그렇지않다.

다른 상황이었다면 다른 알고리즘이 더 좋았을 수도 있다.

2d fussed lasso problem:

- Specialized ADMM: fast(structured subproblems)
- Proximal gradient: slow(poor conditioning)
- Coordinate descent: slow(large active set)

---

# Example: testing changepoints from the 1d fused lasso

조교중 한명이 제안한것이라고 한다.

1d fussed lasso or 1d total variation desnoising

$$\$$
