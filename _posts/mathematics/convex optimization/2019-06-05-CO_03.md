---
title: Convex Optimization - 03 Convexity - Optimization basics
date: 2019-06-05 10:03:00
categories:
  - mathematics
tags:
  - linear algebra
---

[2016 Lec Videos](https://www.youtube.com/watch?v=Gij3dlqLUN8&index=3&list=PLjbUi5mgii6AVdvImLB9-Hako68p9MpIC)

[2016 Slides](http://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/convex-opt.pdf)

---

Least Square Loss는 언제나 Convex하다.

y-Ax 의 제곱이 Least Square Loss인데, 이를 제곱하면 Quadratic Function이 되고, Quadratic Function의 최고차항의 계수가 $$A^TA$$, 즉 Positive Semi Definite이다.

Quadratic Function의 최고차항의 계수가 PSD이면 늘 convex하다.

# Optimization terminology

min f(x)  
x in D  

subject to 

g(x) <= 0  
Ax = b

f는 criterion, objective function

g는 inequality constraint

g를 만족하는 x에 대하여 Ax=b라면 feasible point

모든 feasible point x에 대해 f(x)의 min을 optimal value라고 하고, f* 로 표기한다.

feasible하고 f(x)=f*를 만족하는 x는 optimal이다. 이는 solution, minimizer라고도 불린다.

x가 feasible하고 f(x) <= f* + epsilon 이라면 x를 $$\epsilon$$-suboptimal이라고 한다.

x가 feasible하고 g(x)=0 이라면 g를 x에서 active하다고 한다.

convex minimization은 concave maximization과 대치될 수 있지만 둘 다 convex optimization이라고 부른다.

---

# Properties and first-order optimality

---

# Equivalent transformations

