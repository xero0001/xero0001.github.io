{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gibbs sampling\n",
    "\n",
    "    x: 주사위 1 의 랜덤 샘플\n",
    "    y: 주사위 1 과 2 의 랜덤 샘플의 합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iter=100):\n",
    "    # initialization\n",
    "    # dosent matter which value x and y have.\n",
    "    x, y = 1, 2\n",
    "    for _ in range(num_iter):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return dict(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2): [27, 30],\n",
       " (1, 3): [28, 20],\n",
       " (1, 4): [22, 24],\n",
       " (1, 5): [30, 31],\n",
       " (1, 6): [34, 32],\n",
       " (1, 7): [43, 28],\n",
       " (2, 3): [36, 28],\n",
       " (2, 4): [26, 37],\n",
       " (2, 5): [27, 28],\n",
       " (2, 6): [23, 34],\n",
       " (2, 7): [27, 35],\n",
       " (2, 8): [33, 34],\n",
       " (3, 4): [27, 27],\n",
       " (3, 5): [24, 36],\n",
       " (3, 6): [35, 32],\n",
       " (3, 7): [33, 31],\n",
       " (3, 8): [29, 27],\n",
       " (3, 9): [19, 24],\n",
       " (4, 5): [28, 21],\n",
       " (4, 6): [19, 33],\n",
       " (4, 7): [32, 27],\n",
       " (4, 8): [30, 20],\n",
       " (4, 9): [28, 22],\n",
       " (4, 10): [25, 23],\n",
       " (5, 6): [29, 29],\n",
       " (5, 7): [23, 17],\n",
       " (5, 8): [38, 31],\n",
       " (5, 9): [22, 30],\n",
       " (5, 10): [29, 31],\n",
       " (5, 11): [24, 27],\n",
       " (6, 7): [21, 26],\n",
       " (6, 8): [26, 23],\n",
       " (6, 9): [27, 18],\n",
       " (6, 10): [17, 21],\n",
       " (6, 11): [32, 35],\n",
       " (6, 12): [27, 28]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    \"\"\"a sample from weights using cumulative prob. function\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "K = 4 # num of topics\n",
    "\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = list(map(len, documents))\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "\n",
    "W = len(distinct_words)\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) / \n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"weight of topic k given a word and a doc\"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def w_topic_given_word(word, as_str=True):\n",
    "    weights = [p_word_given_topic(word, k) for k in range(K)]\n",
    "    if as_str:\n",
    "        weights = ['%.4f' % w for w in weights]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0561', '0.0054', '0.0466', '0.0561']\n",
      "['0.0051', '0.0079', '0.0509', '0.0761']\n",
      "['0.0509', '0.0041', '0.0060', '0.1129']\n",
      "['0.1084', '0.0039', '0.0094', '0.0060']\n",
      "['0.1505', '0.0046', '0.0051', '0.0051']\n",
      "['0.1047', '0.0057', '0.0057', '0.0060']\n",
      "['0.1047', '0.0044', '0.0060', '0.0079']\n",
      "['0.1084', '0.0051', '0.0049', '0.0079']\n",
      "['0.1260', '0.0068', '0.0046', '0.0049']\n",
      "['0.1047', '0.0051', '0.0074', '0.0054']\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "n_iter = 1000\n",
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for i_iter in range(n_iter):\n",
    "\n",
    "    if i_iter % 100 == 0:\n",
    "        print(w_topic_given_word('statistics'))\n",
    "\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d], document_topics[d])):\n",
    "            \n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regression', 3),\n",
       " ('Python', 2),\n",
       " ('R', 2),\n",
       " ('libsvm', 2),\n",
       " ('scikit-learn', 2),\n",
       " ('mathematics', 1),\n",
       " ('support vector machines', 1),\n",
       " ('Haskell', 1),\n",
       " ('Mahout', 1),\n",
       " ('Java', 0),\n",
       " ('Cassandra', 0),\n",
       " ('MongoDB', 0),\n",
       " ('Postgres', 0),\n",
       " ('scipy', 0),\n",
       " ('statsmodels', 0),\n",
       " ('probability', 0),\n",
       " ('machine learning', 0),\n",
       " ('statistics', 0),\n",
       " ('C++', 0),\n",
       " ('artificial intelligence', 0),\n",
       " ('HBase', 0),\n",
       " ('NoSQL', 0),\n",
       " ('numpy', 0),\n",
       " ('theory', 0),\n",
       " ('Hadoop', 0),\n",
       " ('Spark', 0),\n",
       " ('Storm', 0),\n",
       " ('pandas', 0),\n",
       " ('programming languages', 0),\n",
       " ('neural networks', 0),\n",
       " ('deep learning', 0),\n",
       " ('decision trees', 0),\n",
       " ('Big Data', 0),\n",
       " ('MapReduce', 0),\n",
       " ('databases', 0),\n",
       " ('MySQL', 0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_counts[2].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
